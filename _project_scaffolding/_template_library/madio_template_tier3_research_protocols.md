## TIER 3 TEMPLATE: RESEARCH PROTOCOLS

**File:** `madio_template_tier3_research_protocols.md`


**Document Authority:** TIER 3 - SUPPORTING SPECIFICATION
**Document Type:** research_protocols
**Version:** 1.0
**Created:** [DATE]
**Last Modified:** [DATE]
**Reports To:** orchestrator

---

## OVERVIEW

**Tier:** 3
**Purpose:** To provide a standardized, systematic protocol for evidence collection and preliminary analysis in any research-intensive project. This template ensures that data gathering is rigorous, reproducible, and provides a solid foundation for subsequent evaluation and root cause analysis.

---

## CRITICAL USAGE INSTRUCTION

This protocol is a mandatory component of **Step 2 (Evidence Collection)** in the `Methodology Framework`. All evidence gathered for an analysis MUST adhere to these guidelines to ensure quality and consistency. The use of a `[PROJECT]_brief` is required to guide the focus of the investigation, but all evidence must be independently collected and verified.

---

## HIERARCHICAL CONTEXT

This Tier 3 `research_protocols` document provides the detailed "how-to" for the evidence-gathering phase of the `Methodology Framework`. It is the foundational data-gathering step upon which the `Evaluation Rubrics` and `Strategic Framework` are applied.

---

## WHEN TO USE

Use this template at the beginning of any project that requires:
- Systematic collection of digital or documented evidence.
- Auditing a collection of assets ([WEBSITES], documents, etc.).
- Standardizing search queries and data extraction methods.
- Ensuring a high-integrity, citable evidence base for analysis.

---

## INTEGRATION REQUIREMENTS

- **Inputs:** Requires a `[PROJECT]_brief` or equivalent document to provide strategic direction, hypotheses, and areas of focus for the investigation.
- **Dependencies:** This is the second step in the `Methodology Framework`, following `Initial Context & Input Capture`.
- **Outputs:** A curated and cited collection of evidence, organized by dimension, ready for evaluation using the `Evaluation Rubrics`.

---

# [PROJECT_NAME]: Evidence Collection Protocol

**Objective:** Systematically gather [DATA_SIGNALS] to enable a partially automated, evidence-based assessment of the [SUBJECT_OF_ANALYSIS].

While a user-provided `[PROJECT]_brief` offers invaluable strategic direction and hypotheses, this Evidence Collection Protocol MUST still be thoroughly and independently executed. The aim is to:
- Validate, verify, update, or challenge the findings and hypotheses presented in the brief using the most current information available.
- Gather fresh, direct, and citable evidence from primary sources for all evaluation dimensions.
- Ensure the entire analysis is based on a current investigation, with all findings attributable to this direct research.

The `[PROJECT]_brief` guides **where** to look and **what** to look for; this protocol is about **gathering the actual evidence**.

---

## SECTION 1: Research & Audit Protocols

### **1.1. Assets to Audit**
- **[PRIMARY_ASSET]:** (e.g., Company homepage, product documentation, core process manual)
- **[COMPONENT_PAGES]:** (e.g., Feature pages, sub-process documents)
- **[PRICING_OR_EFFORT_PAGE]:** (e.g., Pricing page, resource allocation table)
- **[ABOUT_SECTION]:** (e.g., About Us page, project charter)
- **[CONTENT_HUB]:** (e.g., Blog, resources section, knowledge base)
- **[THIRD_PARTY_REVIEW_SITES]:** (e.g., G2, Capterra, industry forums)
- **[PRIMARY_SOCIAL_MEDIA_PRESENCE]:** (e.g., LinkedIn, Twitter/X, community forums)
- **[PUBLIC_COVERAGE]:** (e.g., Press coverage, analyst mentions)
- **[ALTERNATIVE_ASSETS]:** (e.g., Competitor websites, alternative process manuals)
- **[HISTORICAL_ARCHIVES]:** (e.g., News archives, internet archive tools, version history) to identify specific, named, previously successful but potentially underleveraged assets, learnings, or strong positive sentiment ('hidden gems').

### **1.2. Recommended Search Queries**
- `"[SUBJECT_NAME]" site:[THIRD_PARTY_REVIEW_SITE].com`
- `"[SUBJECT_NAME]" [PRICING_TERM]`
- `"[SUBJECT_NAME]" use cases`
- `"[SUBJECT_NAME]" + [CATEGORY_NAME] + [ANALYST_FIRM_NAME]`
- `"[SUBJECT_NAME]" site:[SOCIAL_MEDIA_SITE].com`
- `"[SUBJECT_NAME]" case studies`
- `"[SUBJECT_NAME]" customer stories`
- `"[CATEGORY_NAME]" comparison`
- `"[SUBJECT_NAME]" + [PAIN_POINT_KEYWORDS]`
- `"[ALTERNATIVE_A]" vs "[ALTERNATIVE_B]"`
- `"[SUBJECT_NAME]" notable past projects`
- `"competitors of [SUBJECT_NAME]"`

### **1.3. AI-Guided Extraction Prompts**
- **Primary Asset Analysis:** "What is the primary value proposition communicated on this [PRIMARY_ASSET]? Is it clear who the target audience is and what pain point is being solved?"
- **Call-to-Action Strength:** "Assess the strength and placement of CTAs on this [ASSET]. Are they aligned with user intent and clear in their wording?"
- **Tone & Voice Audit:** "Describe the tone and voice of the brand based on this [ASSET]. Is it consistent with best practices for [TARGET_PERSONA]?"
- **Third-Party Review Synthesis:** "Summarize common themes in reviews for [SUBJECT_NAME]. What are perceived strengths and areas for improvement?"
- **Competitive Messaging Comparison:** "Compare the messaging of [SUBJECT_NAME] with [ALTERNATIVE_NAME]. What are the major similarities and points of differentiation?"
- **Category Definition Check:** "Does the [PRIMARY_ASSET] clearly define the [CONCEPT] or category they play in? If so, what terminology do they use to frame it?"
- **Historical Asset Review Prompt:** "Review archived news, blogs, and other historical records for [SUBJECT_NAME]. Identify any specific, named initiatives, projects, or achievements that received positive engagement but appear to be underleveraged in current materials."

---

## SECTION 2: Root Cause Analysis Framework (Preliminary)

**Objective:** Connect observed symptoms to their underlying strategic causes to guide accurate diagnosis.

### **2.1. Symptom Recognition**
Symptoms may include:
- Weak or generic messaging
- High exit rate or low conversion from key [ASSETS/PAGES]
- Lack of third-party validation or search visibility
- Misaligned CTAs or missing user journey content
- Inconsistent persona targeting
- Low differentiation from [ALTERNATIVES]

### **2.2. Map Symptom to Potential Strategic Cause**
Use this logic to form initial hypotheses. The `[PROJECT]_brief` may provide evidence for these or suggest new root causes.

| Symptom | Possible Root Cause |
| :--- | :--- |
| Generic messaging on [PRIMARY_ASSET] | Undefined or fragmented [USER_PROFILE]; unclear value narrative |
| High exit rate on key pages | Poor CTA clarity; lack of role-aligned conversion paths |
| No clear [CONCEPT]/category definition | Missing category strategy; passive market framing |
| Inconsistent messaging across assets | Lack of brand standards or internal alignment |
| No content for user journey | No user journey mapping or persona clarity |
| Low visibility on search or review sites | Weak content operations or absence of an optimization strategy |
| Overlap in claims with [ALTERNATIVES] | Undifferentiated positioning or insufficient competitive analysis |
| Poor visibility in AI-generated summaries | Weak signals for AI engines; unstructured or unclear content |
| Project stalls due to internal resistance | Messaging fails to address internal detractor concerns |
| High drop-off rate before key action | Lack of diverse/low-friction early-stage engagement options |
| Efforts show limited ROI despite good individual metrics | Failure to leverage 'hidden gems' â€“ proven past successes are 'buried' |

---

## SUCCESS METRICS

- **Completeness:** 100% of the assets listed in the `Assets to Audit` checklist are reviewed for every project.
- **Reproducibility:** An independent researcher following this protocol can reproduce >90% of the collected evidence.
- **Citation Quality:** All collected evidence is cited correctly according to the specified format, with zero citation errors in the final report.

---

## QUALITY CHECKLIST

- [ ] Have all assets from the audit checklist been collected and reviewed?
- [ ] Has each piece of evidence been properly timestamped and cited?
- [ ] Have the recommended search queries been performed and the results documented?
- [ ] Have the AI-guided extraction prompts been used to analyze key artifacts?
- [ ] Have any major information gaps been identified and flagged?
- [ ] Is all collected evidence organized by the [N] assessment dimensions, ready for Step 3 (Evaluation)?
