## TIER 3 TEMPLATE: RUBRICS EVALUATION

**File:** `madio_template_tier3_rubrics_evaluation.md`


**Document Authority:** TIER 3 - SUPPORTING SPECIFICATION
**Document Type:** rubrics_evaluation
**Version:** 1.0
**Created:** [DATE]
**Last Modified:** [DATE]
**Reports To:** orchestrator

---

## OVERVIEW

**Tier:** 3
**Purpose:** To provide a standardized, multi-level rubric for evaluating the effectiveness and quality of a [PROJECT], [SYSTEM], or [PROCESS]. This template defines [N] core dimensions of assessment and a 4-level rating scale to ensure consistent, evidence-based evaluation.

---

## CRITICAL USAGE INSTRUCTION

This rubric is the mandatory evaluation tool for Step 3 of the `Methodology Framework`. When performing an evaluation, you MUST use these dimensions and rating criteria. Ratings for each element must be supported by specific, citable evidence collected during the `Evidence Collection Protocol` (Step 2 of the Methodology).

---

## HIERARCHICAL CONTEXT

This Tier 3 `rubrics_evaluation` document is a dependency for the Tier 3 `methodology_framework`. It provides the specific criteria needed to complete the "Dimension Evaluation" step. It is also informed by the `Strategic Framework` for identifying cross-dimensional strategic issues.

---

## WHEN TO USE

Use this template to:
- Evaluate the performance of a [SYSTEM] against a set of standardized criteria.
- Score the effectiveness of a [PROCESS] or [PROJECT_OUTPUT].
- Provide a consistent, evidence-based foundation for the `Root Cause Analysis` step in the `Methodology Framework`.
- Standardize quality assessment across multiple projects or teams in any [DOMAIN].

---

## INTEGRATION REQUIREMENTS

- **Inputs:** Requires the collected evidence and artifacts from Step 2 of the `Methodology Framework`. May also require context from a `[PROJECT]_brief` to inform the nuance of ratings.
- **Dependencies:** Is a core component of the `Methodology Framework` and must be used to complete Step 3.
- **Outputs:** The completed rubrics (filled-out tables for each dimension) are the primary input for the `Root Cause Analysis` (Step 4) and `Strategic Recommendations` (Step 5) of the methodology.

---

# [PROJECT_NAME]: [N]-Dimension Evaluation Rubrics

## How to Use These Rubrics

This rubric document contains detailed evaluation criteria for the [N] dimensions assessed in the `Methodology Framework`.

1.  Follow the step-by-step instructions in the `Methodology Framework` document.
2.  Use this rubric to perform dimension evaluations in **Step 3**.
3.  Reference specific criteria when documenting strengths and opportunities.
4.  Use the completed rubrics to identify patterns for `Root Cause Analysis` (**Step 4**).
5.  Ensure `Strategic Recommendations` (**Step 5**) address gaps identified using these rubrics.

## The 4-Level Rating System

| Rating | Description |
| :--- | :--- |
| **Exceptional** | Exceeds standards and demonstrates market-leading or best-in-class execution that creates a competitive advantage or significant positive impact. |
| **Competent** | Meets all essential standards for the evaluation element. The execution is solid, effective, and reliable. |
| **Needs Improvement** | Shows inconsistent execution or contains clear, addressable opportunities for improvement. May meet some criteria but fails on others. |
| **Critical Gap** | Demonstrates a complete absence of the element, or the execution is fundamentally flawed, creating significant risk or negative impact. |

---

## The [N] Assessment Dimensions

1.  **[DIMENSION_1_NAME]: Core Concept & Messaging**
2.  **[DIMENSION_2_NAME]: User Journey & Experience**
3.  **[DIMENSION_3_NAME]: Presence & Visibility**
4.  **[DIMENSION_4_NAME]: Audience Clarity & Segmentation**
5.  **[DIMENSION_5_NAME]: [SYSTEM/INTERFACE] Effectiveness**
6.  **[DIMENSION_6_NAME]: Competitive/Alternative Positioning**
7.  **[DIMENSION_7_NAME]: Brand & Message Consistency**
8.  **[DIMENSION_8_NAME]: Analytics & Measurement**
9.  **[DIMENSION_9_NAME]: [TECHNOLOGY]-Specific Authenticity**

---

## Rubric Table: [DIMENSION_1_NAME] - Core Concept & Messaging
**Assessment Focus:** Clarity, distinctiveness, and resonance of the core value proposition in addressing target user needs.

| Element | Exceptional | Competent | Needs Improvement | Critical Gap |
| :--- | :--- | :--- | :--- | :--- |
| **[ELEMENT_1_1]: Value Proposition Clarity** | Value proposition is immediately clear, differentiated, and focused on [USER_OUTCOME]. | Value proposition is reasonably clear but may lack some differentiation. | Value proposition is vague or focused on features rather than outcomes. | Value proposition is absent, confusing, or misaligned with user needs. |
| **[ELEMENT_1_2]: Pain Point Articulation** | Pain points are vividly described using customer language with quantified impact. | Pain points are identified but with limited quantification or resonance. | Pain points are generic or not well-connected to the solution. | Pain points are missing or fundamentally misunderstood. |
| **[ELEMENT_1_3]: Differentiation Framework** | Clear, compelling differentiation from alternatives with substantiated claims. | Basic differentiation that relies primarily on feature comparison. | Weak differentiation that fails to establish meaningful uniqueness. | No meaningful differentiation from alternatives. |
| **[ELEMENT_1_4]: Category Definition** | Creates or clearly owns a distinct category and supports it with extensive educational content. | Fits clearly within an established category but doesn't lead it. | Category placement is ambiguous or inconsistently communicated. | Category confusion or inappropriate category placement. |

---

## Rubric Table: [DIMENSION_2_NAME] - User Journey & Experience
**Assessment Focus:** Cohesion and coverage of the full-funnel experience guiding the user from awareness to action.

| Element | Exceptional | Competent | Needs Improvement | Critical Gap |
| :--- | :--- | :--- | :--- | :--- |
| **[ELEMENT_2_1]: Awareness Stage Content** | Rich, diverse content targeted at problem recognition for different personas. | Basic awareness content that addresses main pain points. | Limited awareness content that reaches only obvious prospects. | No dedicated awareness-stage content. |
| **[ELEMENT_2_2]: Consideration Resources** | Comprehensive resources helping users evaluate options and build a business case. | Standard comparison content and basic evaluation tools. | Limited comparison resources or evaluation support. | No consideration phase support content. |
| **[ELEMENT_2_3]: Decision Facilitation** | Sophisticated tools to reduce friction and support consensus building. | Basic tools that facilitate individual decision-making. | Limited decision support focused primarily on [PRICE/EFFORT]. | No specific decision facilitation tools. |
| **[ELEMENT_2_4]: Engagement Diversity** | Multiple engagement options tailored to user preferences and commitment levels. | Several engagement options but limited personalization. | Few engagement options, mostly high-commitment. | Single engagement path requiring high commitment. |

---

## Rubric Table: [DIMENSION_3_NAME] - Presence & Visibility
**Assessment Focus:** Strength of the brand footprint, discoverability, and third-party validation signals.

| Element | Exceptional | Competent | Needs Improvement | Critical Gap |
| :--- | :--- | :--- | :--- | :--- |
| **[ELEMENT_3_1]: Search Visibility** | Dominant rankings for category and problem-based queries. | Solid rankings for some key terms but gaps in coverage. | Poor search visibility on key terms. | Poor search visibility even for branded terms. |
| **[ELEMENT_3_2]: Third-Party Validation** | Extensive reviews across platforms with strong ratings and case studies. | Moderate review presence with generally positive sentiment. | Limited reviews primarily on a single platform. | No visible third-party validation. |
| **[ELEMENT_3_3]: Media & Analyst Coverage** | Regular, substantive coverage in industry publications and analyst reports. | Some media mentions and occasional analyst recognition. | Limited coverage primarily through self-placed content. | No significant media or analyst coverage. |
| **[ELEMENT_3_4]: Social Proof Diversity** | Rich mix of testimonials, case studies, statistics, and social signals. | Standard testimonials and basic case studies. | A single type of social proof is used. | No meaningful social proof elements. |

---

## SUCCESS METRICS

- **Consistency:** Over 95% of evaluations use this rubric, confirmed by `orchestrator` review.
- **Reliability:** Inter-rater reliability for dimension scores is above 85% when tested.
- **Actionability:** Over 80% of "Needs Improvement" or "Critical Gap" findings are translated into a strategic recommendation in Step 5 of the methodology.

---

## QUALITY CHECKLIST

- [ ] All [N] dimensions are defined with clear assessment focus statements.
- [ ] Each dimension includes a rubric table with 4-5 relevant evaluation elements.
- [ ] Each rubric table uses the standard 4-level rating scale.
- [ ] All placeholder content (`[PLACEHOLDER]`) is clearly marked for replacement.
- [ ] The language is generalized to be applicable to any [DOMAIN].
- [ ] The document correctly references the `Methodology Framework` and `Strategic Framework`.
- [ ] The purpose and integration requirements are clearly stated.
